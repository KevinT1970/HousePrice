df_bankmkt_poutcome <- factor(df_bankmkt$poutcome, levels = c("success","failure",
"other", "unknown"),
labels = c(1,2,3,4))
#change factor to numeric, and omit NA
df_bankmkt_poutcome_num <-na.omit(as.numeric(df_bankmkt_poutcome))
#calculate the means of job, except 'unknown' which is 12
mean(df_bankmkt_poutcome_num[df_bankmkt_poutcome_num != 4])
#replace the 4 to floor(mean), which is 2
df_bankmkt_poutcome_num[df_bankmkt_poutcome_num==4]<-2
df_bankmkt$poutcome  <- df_bankmkt_poutcome_num
#e) default
#change default to factor
df_bankmkt_default <- factor(df_bankmkt$default, levels = c("no","yes"),
labels = c(0,1))
df_bankmkt_default_num <-na.omit(as.numeric(df_bankmkt_default))
#replace default with number
df_bankmkt$default  <- df_bankmkt_default_num
#f) housing
#change housing to factor
df_bankmkt_housing <- factor(df_bankmkt$housing, levels = c("no","yes"),
labels = c(0,1))
#calculate the NA nubmer, if is 0, no necissary to replace
sum(is.na(df_bankmkt_housing)) # the result is 0.
df_bankmkt_housing_num <-na.omit(as.numeric(df_bankmkt_housing))
#replace default with number
df_bankmkt$housing  <- df_bankmkt_housing_num
#g) loan
#change loan to factor
df_bankmkt_loan <- factor(df_bankmkt$loan, levels = c("no","yes"),
labels = c(0,1))
#calculate the NA nubmer, if is 0, no necissary to replace
sum(is.na(df_bankmkt_loan)) # the result is 0.
df_bankmkt_loan_num <-na.omit(as.numeric(df_bankmkt_loan))
#replace default with number
df_bankmkt$loan  <- df_bankmkt_loan_num
# na.omit(cor(df_bankmkt$age, df_bankmkt$balance, method = "pearson"))
# cor(df_bankmkt$loan, df_bankmkt$duration, method = "pearson")
var <- c("job",	"marital",	"education",	"default",	"housing",	"loan",	"duration",	"poutcome")
pairs(df_bankmkt[var])
var <- c("job",	"marital",	"education",	"default",	"housing")
pairs(df_bankmkt[var])
var <- c("job",	"marital",	"education")
pairs(df_bankmkt[var])
var <- c("job",	"marital")
pairs(df_bankmkt[var])
df_bankmkt_education <- factor(df_bankmkt$education, levels = c("primary", "secondary",
"tertiary", "unknown"),
labels = c(1,2,3,4))
#change factor to numeric, and omit NA
df_bankmkt_education_num <-na.omit(as.numeric(df_bankmkt_education))
#calculate the means of job, except 'unknown' which is 12
mean(df_bankmkt_education_num[df_bankmkt_education_num != 4])
#replace the 4 to floor(mean), which is 2
df_bankmkt_education_num[df_bankmkt_education_num==4]<-2
df_bankmkt$education  <- df_bankmkt_education_num
# na.omit(cor(df_bankmkt$age, df_bankmkt$balance, method = "pearson"))
# cor(df_bankmkt$loan, df_bankmkt$duration, method = "pearson")
var <- c("job",	"marital",	"education",	"default",	"housing",	"loan",	"duration",	"poutcome")
pairs(df_bankmkt[var])
#bank5 not include the outlier data
write.csv(df_bankmkt, file = "E:/18 Data Analysis/05 Ryerson DA program/CIND 119 Introduction Big Data Analyst/last project/project_datasets/bank_marketing/bank5.csv")
knitr::opts_chunk$set(echo = TRUE)
lstPackages <- c('shiny', 'shinythemes', 'arules', 'arulesViz', 'gtools', 'klaR')
lapply(lstPackages, library, character.only = TRUE)
sampleData <- read.csv(file = 'SampleObesityData.csv', header = T, stringsAsFactors=T)
summary(sampleData)
str(sampleData)
normalWeightRules <- arules::apriori(sampleData,
parameter =
list(support = 0.04, confidence = 0.5),
appearance =
list(rhs=c("Obesity_Level=Normal_Weight")))
arulesViz::ruleExplorer(normalWeightRules)
obesityIIIRules <- arules::apriori(sampleData,
parameter =
list(support = 0.15, confidence = 0.99),
appearance =
list(rhs=c("Obesity_Level=Obesity_Type_III")))
arulesViz::ruleExplorer(obesityIIIRules)
library("rpart")
library('rpart.plot')
library(klaR)
library(cluster)
lstPackages <- c('klaR', 'cluster')
lapply(lstPackages, library, character.only = TRUE)
clustModel <- klaR::kmodes(sampleData, modes = 7)
cluster::clusplot(sampleData, clustModel$cluster, color=TRUE,  shade=TRUE)
cluster.output <- cbind(sampleData, clustModel$cluster)
table(cluster.output$`clustModel$cluster`)
table(sampleData$Obesity_Level,clustModel$cluster)
x <- c(5, 10, 15)
perm <- permutations(length(x), 2,x, set = TRUE,repeats.allowed = TRUE)
perm
mean_perm <- rowMeans(perm, na.rm = TRUE, dims = 1)
mean_perm
mean(mean_perm)
mean(x)
hist(mean_perm, probability = TRUE, breaks = c(0:20))
dev.off
hist(mean_perm)
dev.off
hist(mean_perm, probability = TRUE, breaks = c(0:20))
par(mfrow=c(2,2))
mean_perm <- rowMeans(perm, na.rm = TRUE, dims = 1)
hist(mean_perm, probability = TRUE, breaks = c(0:20))
hist(mean_perm)
mean_perm <- rowMeans(perm, na.rm = TRUE, dims = 1)
hist(mean_perm)
hist(mean_perm, probability = TRUE, breaks = c(0:20))
obesityIIIRules <- arules::apriori(sampleData,
parameter =
list(support = 0.15, confidence = 0.99),
appearance =
list(rhs=c("Obesity_Level=Obesity_Type_III")))
arulesViz::ruleExplorer(obesityIIIRules)
library("rpart")
library('rpart.plot')
classData <- subset(sampleData, Obesity_Level == 'Normal_Weight' | Obesity_Level == 'Obesity_Type_III')
classData$Obesity_Level <- as.factor(as.character(classData$Obesity_Level))
summary(classData)
str(classData)
#----------
# set seed to make the partition reproducible
set.seed(17)
trainIndex <- sample(seq_len(nrow(classData)), size = floor(0.7 * nrow(classData)))
trainSet <- classData[trainIndex, ]
testSet <- classData[-trainIndex, ]
# Buidling the Predictive Models
fitModel1 <- rpart::rpart(
Obesity_Level ~ Age + Physical_Activity_Frequency + Overweight_Family_History,
data = trainSet,
method = "class",
maxdepth = 10,
parms = list(split = 'information')
)
fitModel2 <- rpart(
Obesity_Level ~  Daily_Water_Consumption + Number_of_main_meals + Alcohol_Consumption,
data = trainSet,
method = "class",
maxdepth = 10,
parms = list(split = 'information')
)
#Plotting the trees
par(mfrow=c(1,2))
rpart.plot::rpart.plot(fitModel1, type = 4, extra = 1)
rpart.plot::rpart.plot(fitModel2, type = 4, extra = 1)
sampleData <- read.csv(file = 'SampleObesityData.csv', header = T, stringsAsFactors=T)
summary(sampleData)
str(sampleData)
normalWeightRules <- arules::apriori(sampleData,
parameter =
list(support = 0.04, confidence = 0.5),
appearance =
list(rhs=c("Obesity_Level=Normal_Weight")))
arulesViz::ruleExplorer(normalWeightRules)
file_path = "E:/18 Data Analysis/05 Ryerson DA program/CIND 119 Introduction Big Data Analyst/last project/project_datasets/bank_marketing/bank.csv"
back_bank_mkt <- read.csv(file = file_path, stringsAsFactors = FALSE,
sep = ",", header = TRUE)
bank_mkt <- back_bank_mkt
print(bank_mkt)
names(df_bankmkt) <- c("age", "job",	"marital",	"education",	"default",	"balance",	"housing",	"loan",	"contact",	"day",	"month",	"duration",	"campaign",	"pdays",	"previous",	"poutcome",	"y")
#statistics for the data
summary(df_bankmkt)
#the structure of the data frame
str(df_bankmkt)
force(array)
apropos(attach())
apropos(att)
apropos("att")
cov((mpg,wt))
cov(mpg,wt)
data("mtcars")
attach(mtcars)
cov(mpg,wt)
cars.test(mpg,wt)
silm <- lm(mpg, wt)
silm <- lm(mpg, wt, data = "mtcars")
silm <- lm(mpg, wt, data = mtcars)
silm <- lm(mpg~ wt, data = mtcars)
summary()
summary(silm)
ppois(3,2,lower.tail = TRUE)
ppois(3,2,t=2,lower.tail = TRUE)
ppois(2,3,lower.tail = TRUE)
ppois(3,2)
ppois(1.5,2)
ppois(40,3)
pbinom(40, 100, 0.6)
dpois(40,3)
pbinom(3,5,0.5)
pbinom(3,5,0.5, lower.tail = FALSE)
rnorm(5)
data()
par(mfrow=c(4,4))
boxplot(LotArea)
boxplot(LotArea)
hist(LotArea)
hist(x = LotArea)
hist(x = HousePrice.LotArea)
hist(x = HousePrice$LotArea)
hist(x = HousePriceDF$LotArea)
hist(x = HousePriceDF.LotArea)
hist(x = na.omit(HousePriceDF.LotArea))
hist(x = na.omit(LotArea))
str(HousePriceDF)
# read the dataset
file_path = "E:/18 Data Analysis/02 Kaggle/01 House Price/train.csv"
HousePriceDF_original <- read.csv(file = file_path, header = TRUE, sep = ',')
str(HousePriceDF_original)
#save the original data, use the backup to analyse
HousePriceDF <- HousePriceDF_original
library(ggplot2)
#the whole numeric data fields saved as original var
var_org <- c("LotFrontage", "LotArea",
"OverallQual","OverallCond",
"YearBuilt","YearRemodAdd",
"MasVnrArea","BsmtFinSF1",
"BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF",
"X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea",
"BsmtFullBath",  "BsmtHalfBath", "FullBath",
"HalfBath", "BedroomAbvGr" , "KitchenAbvGr", "TotRmsAbvGrd",
"Fireplaces", "GarageYrBlt", "GarageCars", "GarageYrBlt", "GarageCars",
"GarageArea","WoodDeckSF", "OpenPorchSF",
"EnclosedPorch", "X3SsnPorch",  "ScreenPorch" ,  "PoolArea",
"MiscVal", "MoSold", "YrSold",
"SalePrice")
#the price components are:
var_4 <- c("LotArea", "OverallQual","OverallCond","BsmtFinSF1",
"TotalBsmtSF","X1stFlrSF", "LowQualFinSF","BsmtFullBath",
"BsmtHalfBath", "FullBath","KitchenAbvGr",
"Fireplaces", "GarageCars", "X3SsnPorch","PoolArea",
"SalePrice")
pairs(HousePriceDF[var_4])
par(mfrow=c(4,4))
hist(x = na.omit(LotArea))
attach(HousePriceDF)
hist(x = na.omit(LotArea))
hist(OverallQual)
hist(OverallCond)
hist(BsmtFinSF1)
hist(BsmtHalfBath)
hist(FullBath)
hist(KitchenAbvGr)
hist(Fireplaces)
hist(TotalBsmtSF)
hist(X1stFlrSF)
hist(LowQualFinSF)
hist(BsmtFullBath)
hist(GarageCars)
hist(X3SsnPorch)
hist(PoolArea)
hist(SalePrice)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
# Write your answer below
avatar <- read_csv("avatar.csv")
# Write your answer below
file_path = "E:/51 Jasmine/11 R/2022-01-11/train.csv"
avatar <- read.csv(file = file_path, header = TRUE, sep = ',')
avatar <- read.csv(file = file_path, header = TRUE, sep = ',',stringsAsFactors = FALSE)
avatar <- read.csv(file = file_path, header = TRUE, sep = ',',stringsAsFactors = FALSE, 'r')
# Write your answer below
file_path = "E:/51 Jasmine/11 R/2022-01-11/avatar.csv"
avatar <- read.csv(file = file_path, header = TRUE, sep = ',',stringsAsFactors = FALSE, 'r')
avatar <- read.csv(file = file_path, header = TRUE, sep = ',',stringsAsFactors = FALSE)
# Write your answer below
glimpse(avatar)
# Write your answer below
file_path = "E:/51 Jasmine/11 R/2022-01-11/avatar.csv"
avatar <- read.csv(file = file_path, header = TRUE, sep = ',',stringsAsFactors = FALSE)
# Tip: don't forget to put quote marks around the name of the dataset inside the function
# Write your answer below
glimpse(avatar)
# Write your answer below
# glimpse(avatar)
head(avatar)
# Row 1 (r1)
r1 <- 6 + 4
# Row 2 (r2)
r2 <- 2 - 3
# Column 1 (c1)
c1 <- 6 / 2
# Column 2 (c2)
c2 <- 4 * 3
my_answers <- c(r1, r2, c1, c2)
square_answers <- c(10, -1, 3, 12)
check <- square_answers == my_answers
sum(check)
# Write your answer below
glimpse(avatar)
# Write your answer below
# glimpse(avatar)
# head(avatar)
getwd()
# Write your answer below
library(tidyverse)
# Write your answer below
library(tidyverse)
# Write your answer below
install.packages("tidyverse")
# Write your answer below
install.packages("tidyverse")
library(tidyverse)
glimpse(avatar)
head(avatar)
# Write your answer below
# install.packages("tidyverse")
library(tidyverse)
glimpse(avatar)
# head(avatar)
# glimpse(avatar)
summary(avatar)
str(avatar)
# glimpse(avatar)
# summary(avatar)
str(avatar)
source('E:/18 Data Analysis/02 Kaggle/01 House Price/tutorial/R/updated-xgboost-with-parameter-tuning.R')
library(MASS)
library(Metrics)
library(corrplot)
library(randomForest)
library(lars)
library(ggplot2)
library(xgboost)
library(Matrix)
library(methods)
library(caret)
install.packages("xgboost")
install.packages("lars")
# install.packages("xgboost")
library(xgboost)
# install.packages("lars")
library(lars)
install.packages("randomForest")
library(randomForest)
Training <- read.csv("../input/train.csv")
Test <- read.csv("../input/test.csv")
# Test whether data is successfully loaded
names(Training)
# ```
# ```{r load}
# Read Data
Training <- read.csv("../../train.csv")
# ```
# ```{r load}
# Read Data
Training <- read.csv("././train.csv")
setwd("d:/Program Files/RStudio")
# ```
# ```{r load}
# Read Data
setwd("E:/18 Data Analysis/02 Kaggle/01 House Price")
Training <- read.csv("train.csv")
Test <- read.csv("test.csv")
# Test whether data is successfully loaded
names(Training)
Num_NA<-sapply(Training,function(y)length(which(is.na(y)==T)))
NA_Count<- data.frame(Item=colnames(Training),Count=Num_NA)
Num_NA
NA_Count
# Among 1460 variables, 'Alley',  'PoolQC', 'Fence' and 'MiscFeature' have amazingly high number of missing value. Therefore, I
# have decided to remove those variables. After that, the number of effective variables has shrunken to 75 (excluding id).
#
# ```{r,message=FALSE, warning=FALSE}
Training<- Training[,-c(7,73,74,75)]
Training.head()
head(Training)
# head(Training)
# ```
# Then, I transferred dummny variables into numeric form. Due to the intimidating size of dummy variables, I decided to transfer them
# directly by implementing 'as.integer' method. This is why I let the string as factor when reading the data file. The numeric variables
# are sorted out in particular for the convenience of descriptive analysis.
#
# ```{r,message=FALSE, warning=FALSE}
# Numeric Variables
Num<-sapply(Training,is.numeric)
Num
Num<-Training[,Num]
Num
Num
for(i in 1:77){
if(is.factor(Training[,i])){
Training[,i]<-as.integer(Training[,i])
}
}
# Test
Training$Street[1:50]
# ```
# Finally, for the remaining missing values, I replaced them with zero directly. The data cleansing procedure ends here.
#
# ```{r,message=FALSE, warning=FALSE}
Training[is.na(Training)]<-0
Num[is.na(Num)]<-0
# We first draw a corrplot of numeric variables. Those with strong correlation with sale price are examined.
# ```{r,message=FALSE, warning=FALSE}
corrplot(correlations, method="circle", type="lower",  sig.level = 0.01, insig = "blank")
# ```
# 'OverallQual','TotalBsmtSF','GarageCars' and 'GarageArea' have relative strong correlation with each other. Therefore, as an example, we plot the correlation
# among those four variables and SalePrice.
# ```{r,message=FALSE, warning=FALSE}
pairs(~SalePrice+OverallQual+TotalBsmtSF+GarageCars+GarageArea,data=Training,
main="Scatterplot Matrix")
corrplot(correlations, method="circle", type="lower",  sig.level = 0.01, insig = "blank")
# We first draw a corrplot of numeric variables. Those with strong correlation with sale price are examined.
# ```{r,message=FALSE, warning=FALSE}
correlations<- cor(Num[,-1],use="everything")
corrplot(correlations, method="circle", type="lower",  sig.level = 0.01, insig = "blank")
# ```
# 'OverallQual','TotalBsmtSF','GarageCars' and 'GarageArea' have relative strong correlation with each other. Therefore, as an example, we plot the correlation
# among those four variables and SalePrice.
# ```{r,message=FALSE, warning=FALSE}
pairs(~SalePrice+OverallQual+TotalBsmtSF+GarageCars+GarageArea,data=Training,
main="Scatterplot Matrix")
# ```
# The dependent variable (SalePrice) looks having decent linearity when plotting with other variables. However, it is also obvious that some independent variables
# also have linear relationship with others. The problem of multicollinearity is obvious and should be treated when the quantity of variables in regression formula is huge.
#
# The final descriptive analysis I put here would be the relationship between the variable 'YearBu' and Sale Price.
#
# ```{r,message=FALSE, warning=FALSE}
p<- ggplot(Training,aes(x= YearBuilt,y=SalePrice))+geom_point()+geom_smooth()
p
# Before implementing models, one should first split the training set of data into 2 parts: a training set within the training set and a test set that can be used for evaluation.
# Personally I prefer to split it with the ratio of 6:4, ***But if someone can tell me what spliting ratio is proved to be scienticfic I will be really grateful***
#
#   ```{r,message=FALSE, warning=FALSE}
# Split the data into Training and Test Set # Ratio: 6:4 ###
Training_Inner<- Training[1:floor(length(Training[,1])*0.6),]
Test_Inner<- Training[(length(Training_Inner[,1])+1):1460,]
summary(reg1)
reg1<- lm(SalePrice~., data = Training_Inner)
# The first and simplest but useful model is linear regression model. As the first step, I put all variables into the model.
# ```{r,message=FALSE,warning=FALSE}
reg1<- lm(SalePrice~., data = Training_Inner)
# R Square is not bad, but many variables do not pass the Hypothesis Testing, so the model is not perfect. Potential overfitting will occur if someone insist on using it. Therefore,
# the variable selection process should be involved in model construction. I prefer to use Step AIC method.
#
# Several variables still should not be involved in model. By checking the result of Hypothesis Test, I mannually build the final linear regression model.
#
# ```{r,message=FALSE,warning=FALSE}
reg1_Modified_2<-lm(formula = SalePrice ~ MSSubClass + LotArea +
Condition2 + OverallQual + OverallCond +
YearBuilt  + RoofMatl +  ExterQual +
BsmtQual + BsmtCond + BsmtFinSF1 + BsmtFinSF2 +
BsmtUnfSF + X1stFlrSF + X2ndFlrSF + BedroomAbvGr + KitchenAbvGr +
KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + FireplaceQu +
GarageYrBlt + GarageCars +  SaleCondition,
data = Training_Inner)
summary(reg1_Modified_2)
# ```
# The R Square is not bad, and all variables pass the Hypothesis Test. The diagonsis of residuals is also not bad. The diagnosis can be viewed below.
# ```{r,message=FALSE,warning=FALSE}
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
plot(reg1_Modified_2)
par(mfrow=c(1,1))
# ```
#
# We check the performance of linear regression model with RMSE value.
#
# ```{r,message=FALSE,warning=FALSE}
rmse(log(Test_Inner$SalePrice),log(Prediction_1))
# ```{r,message=FALSE,warning=FALSE}
Independent_variable<- as.matrix(Training_Inner[,1:76])
Dependent_Variable<- as.matrix(Training_Inner[,77])
laa<- lars(Independent_variable,Dependent_Variable,type = 'lasso')
plot(laa)
# The plot is messy as the quantity of variables is intimidating. Despite that, we can still use R to find out the model with least multicollinearity. The selection
# procedure is based on the value of Marrow's cp, an important indicator of multicollinearity. The prediction can be done by the script-chosen best step and RMSE can be used
# to assess the model.
#
# ```{r,message=FALSE,warning=FALSE}
best_step<- laa$df[which.min(laa$Cp)]
Prediction_2<- predict.lars(laa,newx =as.matrix(Test_Inner[,1:76]), s=best_step, type= "fit")
rmse(log(Test_Inner$SalePrice),log(Prediction_2$fit))
# The other model I chose to fit in the training set is Random Forest model. The model, prediction and RMSE calculation can be found below:
#
# ```{r,message=FALSE, warning=FALSE}
for_1<- randomForest(SalePrice~.,data= Training_Inner)
Prediction_3 <- predict(for_1, newdata= Test_Inner)
rmse(log(Test_Inner$SalePrice),log(Prediction_3))
# This amazing package really impressed me! And I have enthusiam to explore it. The first step of XGBoost is to transform the dataset into Sparse matrix.
#
# ```{r,message=FALSE,warning=FALSE}
train<- as.matrix(Training_Inner, rownames.force=NA)
test<- as.matrix(Test_Inner, rownames.force=NA)
train <- as(train, "sparseMatrix")
test <- as(test, "sparseMatrix")
# Never forget to exclude objective variable in 'data option'
train_Data <- xgb.DMatrix(data = train[,2:76], label = train[,"SalePrice"])
# ```
# Then I tune the parameters of xgboost model by building a 20-iteration for-loop. **Not sure whether this method is reliable but really time-consuming**
# **Updated** Thanks for the advices from my fellow Kaggle friend! Now I understand how to use 'Caret' to perform grid search for the parameters.
# ```{r,message=FALSE,warning=FALSE}
# Tuning the parameters #
cv.ctrl <- trainControl(method = "repeatedcv", repeats = 1,number = 3)
xgb.grid <- expand.grid(nrounds = 500,
max_depth = seq(6,10),
eta = c(0.01,0.3, 1),
gamma = c(0.0, 0.2, 1),
colsample_bytree = c(0.5,0.8, 1),
min_child_weight=seq(1,10)
)
xgb_tune <-train(SalePrice ~.,
data=Training_Inner,
method="xgbTree",
metric = "RMSE",
trControl=cv.ctrl,
tuneGrid=xgb.grid
)
print(xgb.grid)
test_data <- xgb.DMatrix(data = test[,2:76])
prediction <- predict(xgb_tune, test_data)
rmse(log(Test_Inner$SalePrice),log(prediction))
